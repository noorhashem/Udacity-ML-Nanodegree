{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Are used to measure how good a model is in classifying the data it handles\n",
    "\n",
    "We use more than one metric \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "Is measuring how many correctly classified points/predictions did our model achieve. It is the ratio of true positives + true negatives records(points) the model did classify / the number of total records (points) we have.\n",
    "\n",
    "Ration between : True Positive + True Negatives / Total \n",
    "\n",
    "Accuracy might be tricky when the difference between the classified categories is skewed (i.e. the difference between the number of credit card transactions that are classified as good transactions and the number of credit card transactions that are classified as fraudulent). When this difference is huge, the accuracy would me misleading as a model like this if it has a high number of good transactions (284,335) and a smaller number of bad transactions (472), and by calculating its accuracy over the total number of transactions it would give us a high accuracy that’s almost 99% while It’s not that accurate of a model because it is almost not catching enough of the bad transactions and the point of this model is to catch as much fraudulent transactions as possible and it’s only catching 472 fraudulent transactions while giving us high accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For understanding Precision and Recall we’d have to understand two aspects that define our model, which are the False Positives and the False Negatives. Whichever one is more critical will help us know which metric is more important for us and more sensitive to our measurements ( Precision or Recall ).  ",
    " ",
    "False Positives: are the cases that the model classifies as positive/true/correct and they are not truly correct. ",
    "False Negatives: are the cases that the model classifies as negative/incorrect/false ",
    " in the light of the classifier but they are not actually negative.\n",
    "\n",
    "In some models, catching false positives is more critical to our scenario than catching false negatives and vice cress. So we need a metric that punishes our model predictions that gives false positives or false negatives depending on the nature of our classifier. ",
    " ",
    "(I.e. If you have a model that classifies people as sick or healthy, It’s more dangerous to classify patients who are sick as healthy people ( classify false negatives) than to classify a person as sick while he is healthy ( a false positive) so you care more about false negatives…then you need a high Recall model. We’ll know why when we define precision and recall and see how they can help.\n",
    "\n",
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision\n",
    "\n",
    "It is the subset of the points/records in your dataset that are truly correct/ positive from all the points that the model classified as correct/positive \n",
    "\n",
    "Ratio between : TruePositives/ TruePositives+FalsePositives (Actual Results)   ",
    " ",
    "So we’re trying to measure the accuracy of our model by putting in consideration the False positives that the model might have classified. So this can give us a more realistic metric for measuring the accuracy of our model where false positives are the most critical that we’re trying to punish in our algorithm.\n",
    "\n",
    "<b>TL;DR</b> : you’re checking, out of the correct strikes your model assumed are correct, which of them were truly correct.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall\n",
    "\n",
    "\n",
    "It is the subset of the points/records in your dataset that the model could predict as positive/correct from the sum of all the other cases/points in the dataset that are actually positive/correct.\n",
    "\n",
    "Ratio between : TruePositives/TruePositives+FalseNegatives (Predicted results) \n",
    "\n",
    "It’s a metric to measure the number of correct cases the model manage to pin point from the actual positive/correct data.</br></br></br>\n",
    "\n",
    "\n",
    "<b>TL;DR</b> : You’re checking which parts of the dataset did your model strike right\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1- Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F-Beta Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
